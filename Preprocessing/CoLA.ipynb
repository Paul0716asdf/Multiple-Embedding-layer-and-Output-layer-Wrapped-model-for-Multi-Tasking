{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gj04</th>\n",
       "      <th>1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Our friends won't buy this analysis, let alone the next one we propose.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'll fix you a drink.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gj04  1 Unnamed: 2  \\\n",
       "0  gj04  1        NaN   \n",
       "1  gj04  1        NaN   \n",
       "2  gj04  1        NaN   \n",
       "3  gj04  1        NaN   \n",
       "4  gj04  1        NaN   \n",
       "\n",
       "  Our friends won't buy this analysis, let alone the next one we propose.  \n",
       "0  One more pseudo generalization and I'm giving up.                       \n",
       "1   One more pseudo generalization or I'm giving up.                       \n",
       "2     The more we study verbs, the crazier they get.                       \n",
       "3          Day by day the facts are getting murkier.                       \n",
       "4                              I'll fix you a drink.                       "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "from typing import *\n",
    "\n",
    "df_train = pd.read_csv(\"cola_public\\\\raw\\\\in_domain_train.tsv\",sep='\\t')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.drop(columns=['gj04', 'Unnamed: 2'])\n",
    "df.columns = ['label', 'context']\n",
    "df.to_csv('CoLA_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoLA_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        df = self.df\n",
    "        EC = self.tokenizer.encode_plus(df['context'][index])\n",
    "\n",
    "        input_ids = torch.tensor(EC['input_ids'])\n",
    "        mask = torch.tensor(EC['attention_mask'])\n",
    "        token = torch.tensor(EC['token_type_ids'])\n",
    "\n",
    "        return input_ids, mask, token, df['label'][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(sample): \n",
    "    input_ids_batch = [s[0] for s in sample]\n",
    "    mask_batch = [s[1] for s in sample]\n",
    "    token_batch = [s[2] for s in sample]\n",
    "    Label_batch = ([s[3] for s in sample])\n",
    "\n",
    "    input_ids_batch = pad_sequence(input_ids_batch, batch_first=True)\n",
    "    mask_batch = pad_sequence(mask_batch, batch_first=True)\n",
    "    token_batch = pad_sequence(token_batch, batch_first=True)\n",
    "\n",
    "    return input_ids_batch, mask_batch, token_batch, Label_batch\n",
    "\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CoLA_Dataset(df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  1284, 10788,  1111, 10095,  1107,  1103,  1298,   119,   102],\n",
       "         [  101,  2159,  3885,  1103,  1148,  2259,  1103,  3240,   119,   102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " [0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('jjason_conda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47cde9b33b7544b02f38b3109f9a57a489f4842f708505745ae5540383cddba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
